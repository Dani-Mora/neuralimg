{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os.path\n",
    "import numpy as np\n",
    "import abc\n",
    "import time\n",
    "import datetime as dt\n",
    "from neuralimg.dataio import *\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_conv_weights(i, ksize, inps, outs, m=0.0, stddev=1e-1, bias=0):\n",
    "    \"\"\" Creates convolutional weights and bias. Initializes the first with a\n",
    "        truncated normal distribution and the second with a constant\n",
    "        :param i: Identifier of the weights\n",
    "        :param ksize: Size of the kernel\n",
    "        :param outs: Depth of the output\n",
    "        :param m: Mean of the initialization of the weights\n",
    "        :param stddev: Standard deviation of the weights initialization\n",
    "        :param bias: Initialization value for the bias\n",
    "    \"\"\"\n",
    "    w = tf.get_variable('convw' + str(i), [ksize, ksize, inps, outs],\n",
    "            initializer = tf.truncated_normal_initializer(mean=m, stddev=stddev))\n",
    "    \n",
    "    b = tf.get_variable('convb' + str(i) , [outs], \n",
    "            initializer = tf.constant_initializer(bias))\n",
    "    return w, b\n",
    "\n",
    "def create_fc_weights(i, inps, hiddens, m=0.0, stddev=1e-1, bias=0):\n",
    "    \"\"\" Creates convolutional weights and bias. Initializes the first with a\n",
    "        truncated normal distribution and the second with a constant\n",
    "        :param i: Identifier of the weights\n",
    "        :param inps: Size of the inputs\n",
    "        :param hiddens: Number of hidden units\n",
    "        :param m: Mean of the initialization of the weights\n",
    "        :param stddev: Standard deviation of the weights initialization\n",
    "        :param bias: Initialization value for the bias\n",
    "    \"\"\"\n",
    "    w = tf.get_variable('fullyw' + str(i), [inps, hiddens],\n",
    "            initializer = tf.truncated_normal_initializer(mean=m, stddev=stddev))\n",
    "    \n",
    "    b = tf.get_variable('fullyb' + str(i) , [hiddens], \n",
    "            initializer = tf.constant_initializer(bias))\n",
    "    return w, b\n",
    "\n",
    "def summary_activations(name, x):\n",
    "    \"\"\" Defines a histogram and a sparsity measure of the activations for the input function \"\"\"\n",
    "    tf.histogram_summary(name + '/activations', x)\n",
    "    tf.scalar_summary(name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "\n",
    "def define_convolution(data, w, b, i, maxp, nr, prefix):\n",
    "    \"\"\" Returns a convolutional layer with the input parameters. Weights are initialized as truncated\n",
    "    normals around 0 and stdev of 1e-3 and bias as 0.\n",
    "        :param data: Input placeholder\n",
    "        :param i: Identifier of the convolutional layer\n",
    "        :param maxp: Whether to perform max pooling after the activation. TODO: check characteristics of max pooling\n",
    "        :param nr: Whether to perform loal response normalisation at the end TODO: check charcateristics of this\n",
    "        :param prefix: Prefix of the layer (left/right/center)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convolution operation\n",
    "    conv = tf.nn.conv2d(data, w, strides=[1, 1, 1, 1], padding='SAME', \n",
    "                         name=prefix + '-conv' + str(i))\n",
    "    \n",
    "    # Perform ReLU(x) = max(0, x)\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, b), name=prefix + '-relu' + str(i))\n",
    "    \n",
    "    # Perform max pooling, if requested\n",
    "    if maxp is True:\n",
    "        relu = tf.nn.max_pool(relu, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], \n",
    "                               padding='SAME', name=prefix + '-pool' + str(i))\n",
    "    \n",
    "    # Perform Local Response Normalisation if requested\n",
    "    if nr is True:\n",
    "        relu = tf.nn.lrn(relu, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=prefix + '-norm' + str(i))  \n",
    "    \n",
    "    # Add summary for all operations\n",
    "    summary_activations(prefix + '-conv' + str(i), relu)\n",
    "    \n",
    "    return relu\n",
    "\n",
    "\n",
    "def define_fully_layer(data, w, b, i, prefix, dropout=None):\n",
    "    \"\"\" Returns a fully connected layer with the input parameters. Weights are initialized as truncated\n",
    "    normals around 0 and stdev of 1e-3 and bias as 0.\n",
    "        :param data: Input placeholder\n",
    "        :param i: Identifier of the convolutional layer\n",
    "        :param prefix: Prefix of the layer (left/right/center)\n",
    "        :param mode: Mode to use (training, validation, testing)\n",
    "        :param dropout: Keep probability of the dropout (in interval [0,1]). None if not used\n",
    "        # TODO: use different activation functions?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform ReLU(x) = max(0, x)\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(tf.matmul(data, w), b), name=prefix + '-relufc' + str(i))\n",
    "    \n",
    "    # Add dropout if requested\n",
    "    if dropout is not None:\n",
    "        relu = tf.nn.dropout(relu, dropout)\n",
    "\n",
    "    summary_activations(prefix + '-relu' + str(i), relu)\n",
    "        \n",
    "    return relu\n",
    "\n",
    "def store_checkpoint(session, saver, step, path, name='model'):\n",
    "    \"\"\" Stores a checkpoint of the model\n",
    "        :param session: Tensorflow session\n",
    "        :param saver: Tensorflow saver\n",
    "        :param step: Training step where the model belongs to\n",
    "        :param path: Folder where to store the model\n",
    "        :param name: Name of the checkpoint file\n",
    "    \"\"\"\n",
    "    checkpoint_path = os.path.join(path, name + '_' + str(step) + '.ckpt')\n",
    "    saver.save(session, checkpoint_path, global_step=step)\n",
    "        \n",
    "def euclidean_dist(f1, f2):\n",
    "    \"\"\" Computes the euclidean distance between two tensors \"\"\"\n",
    "    # Must be float, convert to be sure\n",
    "    f1 = tf.cast(f1, tf.float32)\n",
    "    f2 = tf.cast(f2, tf.float32)\n",
    "        \n",
    "    # Compute euclidean distance between outputs\n",
    "    subs = tf.sub(f1, f2)       \n",
    "    pows = tf.pow(subs, tf.constant(2, tf.float32, shape=subs.get_shape()))\n",
    "    suma = tf.reduce_sum(pows, 1)\n",
    "    return  tf.sqrt(suma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: options can be parsed from a file\n",
    "\n",
    "class SiameseNetwork(object):\n",
    "    \n",
    "    __metaclass__ = abc.ABCMeta\n",
    "    \n",
    "    def __init__(self, opts):\n",
    "        self.opts = opts\n",
    "        self.scope_name = 'siamese'\n",
    "        self.loss_interval = 10\n",
    "        self.summary_interval = 2\n",
    "        self.checkp_interval = 1000\n",
    "        self.train_loss = []\n",
    "        self.val_loss = []\n",
    "        self.check_options()\n",
    "        self.val_track = []\n",
    "        \n",
    "    @abc.abstractmethod\n",
    "    def get_placeholder_suffixes(self):\n",
    "        \"Returns the placeholders according to the subclass \"\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_loss(self, placeholders):\n",
    "        \" Defines the specific loss function to use\"\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def get_label_batch(self, offset, batch_size):\n",
    "        \" Returns the labels for the corresponding offset \"\n",
    "    \n",
    "    \n",
    "    def check_options(self):\n",
    "        \"\"\" Parser the options \"\"\"\n",
    "        self.keep_prob = self.opts['dropout'] if 'dropout' in self.opts else None\n",
    "        self.early = self.opts['early'] if 'early' in self.opts else None\n",
    "        self.track_val = [] if 'early' in self.opts else None\n",
    "        \n",
    "        \n",
    "    def read_data(self, path):\n",
    "        \"\"\" Reads the data from the input dataset\n",
    "            :param path: Path to the HDF5 containing the data. Must have a group for\n",
    "                each set: validation, training and testing with datasets 'data' and 'labels'\n",
    "        \"\"\"\n",
    "        dr = DatasetReader(path)\n",
    "        data = dr.read()\n",
    "        self.tr_data = reshape_data(data['training']['data'])\n",
    "        self.tr_labels = data['training']['labels']\n",
    "        self.val_data = reshape_data(data['validation']['data'])\n",
    "        self.val_labels = data['validation']['labels']\n",
    "        self.test_data = reshape_data(data['testing']['data'])\n",
    "        self.test_labels = data['testing']['labels']\n",
    "        self.insts, self.imh, self.imw, self.depth = self.tr_data.shape[1:]\n",
    "        \n",
    "        \n",
    "    def _define_weights(self):\n",
    "        \"\"\" Defines the weights needed in he network \"\"\"\n",
    "        # TODO: do it in an automatic way\n",
    "        wc1, bc1 = create_conv_weights(1, self.opts['ksize'], 3, self.opts['maps'])\n",
    "        wc2, bc2 = create_conv_weights(2, self.opts['ksize'], self.opts['maps'], self.opts['maps'])\n",
    "        wc3, bc3 = create_conv_weights(3, self.opts['ksize'], self.opts['maps'], self.opts['maps'])\n",
    "        \n",
    "        # Number dividing depends on number of convolutions\n",
    "        wf1, bf1 = create_fc_weights(1, self.imh // 8 * self.imw // 8 * self.opts['maps'], self.opts['hiddens1'])\n",
    "        wf2, bf2 = create_fc_weights(2, self.opts['hiddens1'], self.opts['hiddens2'])\n",
    "        return [[wc1, bc1], [wc2, bc2], [wc3, bc3], [wf1, bf1], [wf2, bf2]]\n",
    "    \n",
    "    \n",
    "    def model(self, images, prefix):\n",
    "        \"\"\" Defines the graph model \n",
    "            :param images: Input data\n",
    "            :param prefix: Prefix to add to each operator to identify elemtn in pair/trilet\n",
    "        \"\"\"\n",
    "        \n",
    "        # Obtain weights (already created)\n",
    "        ws = self._define_weights()\n",
    "        \n",
    "        tf.image_summary(prefix, images, max_images=3)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        with tf.variable_scope(\"conv1_\" + prefix) as scope:\n",
    "            norm1 = define_convolution(images, ws[0][0], ws[0][1], 1, \n",
    "                                       self.opts['maxp'], self.opts['lrn'], prefix)\n",
    "            \n",
    "        with tf.variable_scope(\"conv2_\" + prefix) as scope:\n",
    "            norm2 = define_convolution(norm1,  ws[1][0], ws[1][1], 2, \n",
    "                                       self.opts['maxp'], self.opts['lrn'], prefix)\n",
    "                                                                            \n",
    "        with tf.variable_scope(\"conv3_\" + prefix) as scope:\n",
    "            norm3 = define_convolution(norm2,  ws[2][0], ws[2][1], 3, \n",
    "                                       self.opts['maxp'], self.opts['lrn'], prefix)\n",
    "        \n",
    "        # Flatten convolutions output\n",
    "        shape = norm3.get_shape().as_list()   \n",
    "        flat = tf.reshape(norm3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        # Fully connected layers\n",
    "        with tf.variable_scope(\"fully1_\" + prefix) as scope:\n",
    "            fc1 = define_fully_layer(flat,  ws[3][0], ws[3][1], 1, prefix, self.keep_prob)\n",
    "        \n",
    "        with tf.variable_scope(\"fully2_\" + prefix) as scope:\n",
    "            fc2 = define_fully_layer(fc1,  ws[4][0], ws[4][1], 2, prefix, self.keep_prob)\n",
    "        \n",
    "        # Store the variables for further examination\n",
    "        self.v = [norm1, norm2, norm3, fc1, fc2]\n",
    "        \n",
    "        return fc2\n",
    "    \n",
    "    def define_placeholders(self):\n",
    "        \"\"\" Initializes all placeholders needed in the networ \"\"\"\n",
    "        self.pls = self.get_data_placeholders() # Data\n",
    "        self.pl_labels = tf.placeholder(tf.float32, shape=(self.opts['batch_size'])) # Labels\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_probability') # Dropout\n",
    "    \n",
    "    def get_data_placeholders(self):\n",
    "        \"\"\" Returns the input placeholders/constants for the data \"\"\"\n",
    "        names = self.get_placeholder_suffixes()\n",
    "        training = [tf.placeholder(tf.float32, shape=(self.opts['batch_size'], self.imh, self.imw, \n",
    "                                                 self.depth), name=i) for i in names]\n",
    "        return training\n",
    "    \n",
    "    def evaluate_net(self, data, labels, session):     \n",
    "        \"\"\" Computes the mean loss for the input setwithout upgrading the weights\n",
    "            Args:\n",
    "                data: Input image data\n",
    "                labels: Input label data\n",
    "                session: Current execution session\n",
    "            Returns:\n",
    "                Loss and duration of the evaluation\n",
    "        \"\"\"\n",
    "        size = data.shape[0]\n",
    "        if size < self.opts['batch_size']:\n",
    "            raise ValueError(\"Batch size for evals larger than dataset: %d\" % size)\n",
    "        \n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        for begin in xrange(0, size, self.opts['batch_size']):\n",
    "            \n",
    "            # Obtain data for current step\n",
    "            end = begin + self.opts['batch_size']\n",
    "            d = data[begin:end, ...] if end <= size else data[-self.opts['batch_size']:, ...]\n",
    "            l = labels[begin:end] if end <= size else labels[-self.opts['batch_size']:]\n",
    "            \n",
    "            # Feed into the network to obtain the loss\n",
    "            loss_val = session.run(self.loss, feed_dict=self.build_feed(d, l))\n",
    "            losses.append(loss_val)\n",
    "            \n",
    "        duration = time.time() - start_time\n",
    "        return numpy.mean(losses), duration\n",
    "\n",
    "    def build_feed(self, batch_data, batch_labels):\n",
    "        \"\"\" Builds a dictionary that feeds the network\"\"\"\n",
    "        feed = {pl: batch_data[:, i, ...] for i, pl in enumerate(self.pls)}\n",
    "        feed.update({self.pl_labels: batch_labels, self.keep_prob: self.opts['dropout']})\n",
    "        return feed\n",
    "    \n",
    "    def perform_training(self, batch_data, batch_labels, session, step):\n",
    "        \"\"\"Performs a training operation and returns the loss and the duration taken.\n",
    "            Args:\n",
    "                batch_data: Input data batch\n",
    "                batch_labels: Input label batch\n",
    "                session: Current execution session\n",
    "                step: Step at which we want to store a summary. None to disable storing\n",
    "            Returns:\n",
    "                Loss and duration of the training\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        feed = self.build_feed(batch_data, batch_labels)\n",
    "        if step is None:\n",
    "            print('yes')\n",
    "            s, l = session.run([self.optimizer, self.loss], feed_dict=feed)\n",
    "            duration = time.time() - start_time\n",
    "        else:\n",
    "            print('no')\n",
    "            summary_str, r, l = session.run([self.summary_op, self.optimizer, self.loss], feed_dict=feed)\n",
    "            duration = time.time() - start_time\n",
    "            self.sum_writer.add_summary(summary_str, step)\n",
    "        return l, duration\n",
    "    \n",
    "    def get_batch(self, step):\n",
    "        \"\"\" Returns a batch from the training data according to the current step\"\"\"\n",
    "        offset = (step * self.opts['batch_size']) % (self.tr_data.shape[0] - self.opts['batch_size'])\n",
    "        batch_data = self.tr_data[offset:(offset + self.opts['batch_size']), ...]\n",
    "        batch_labels = self.get_label_batch(offset, self.opts['batch_size'])\n",
    "        return batch_data, batch_labels\n",
    "    \n",
    "    def track_validation(self, step, new_value, session, saver, outp):\n",
    "        \"\"\" Tracks the window of validation losses and checks whether the new value \"\"\"\n",
    "        if len(self.track_val) < self.early:\n",
    "            self.track_val.append(new_value)\n",
    "            return False\n",
    "        if self.track_val[0] < new_value:\n",
    "            return True\n",
    "        else:\n",
    "            self.track_val = self.track[1:] + [new_value]\n",
    "            store_checkpoint(session, saver, step, outp, 'early')\n",
    "            return False\n",
    "    \n",
    "    def train(self, outp):\n",
    "        \"\"\" Buils the network given the read data and starts training\n",
    "            :param out: Where to store the resulting model and the checkpoints\n",
    "        \"\"\"\n",
    "        \n",
    "        # Restore from past checkpoint - TODO\n",
    "        \n",
    "        dev = 'gpu' if self.opts['gpu'] is True else 'cpu'\n",
    "        g = tf.Graph()\n",
    "\n",
    "        with g.as_default(), tf.device('/' + dev + ':0'):\n",
    "            \n",
    "            with tf.variable_scope(self.scope_name) as scope:\n",
    "                \n",
    "                # Create shared weights in the root scope\n",
    "                ws = self._define_weights()\n",
    "                scope.reuse_variables()\n",
    "                \n",
    "                self.define_placeholders()\n",
    "                \n",
    "                # Define losses and set gradient descent to minimize it\n",
    "                self.loss = self.get_loss(self.pls, self.pl_labels)\n",
    "                self.optimizer = tf.train.GradientDescentOptimizer(self.opts['lr']).minimize(self.loss)\n",
    "    \n",
    "                # Define summaries for all variables\n",
    "                for var in tf.trainable_variables():\n",
    "                    tf.histogram_summary(var.op.name, var)\n",
    "    \n",
    "            with tf.Session(graph=g, config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "\n",
    "                # Initialize all defined variables\n",
    "                tf.initialize_all_variables().run()\n",
    "                \n",
    "                # Prepare summaries for Tensorboard visualization\n",
    "                self.summary_op = tf.merge_all_summaries()\n",
    "                self.sum_writer = tf.train.SummaryWriter(self.opts['logs'], session.graph)             \n",
    "                saver = tf.train.Saver()\n",
    "\n",
    "                for step in range(1, self.opts['steps'] + 1):\n",
    "                    self._training_step(session, step, saver, outp)\n",
    "                \n",
    "                store_checkpoint(session, saver, step, outp, name='final')\n",
    "                \n",
    "            \n",
    "    def _training_step(self, session, step, saver, outp):\n",
    "        \"\"\" Performs a single training step \"\"\"\n",
    "\n",
    "        tr_data, tr_labels = self.get_batch(step)\n",
    "\n",
    "        # Update gradients. Each 5 steps (default, TODO change to 100) also write summary of operations\n",
    "        s = None if step % self.summary_interval != 0 else step\n",
    "        loss_value, duration = self.perform_training(tr_data, tr_labels, session, s)\n",
    "        \n",
    "        # Print loss each 10 steps (default)\n",
    "        if step % self.loss_interval == 0:\n",
    "\n",
    "            # Validation - Check loss evolution without updating gradients\n",
    "            lv, durationv = self.evaluate_net(self.val_data, self.val_labels, session)\n",
    "            \n",
    "            assert not np.isnan(lv)\n",
    "\n",
    "            # Print and save losses\n",
    "            self.print_loss(loss_value, step, 'training', duration)\n",
    "            self.print_loss(lv, step, 'validation', durationv)\n",
    "            self.train_loss.append(loss_value)\n",
    "            self.val_loss.append(lv)\n",
    "            \n",
    "            if self.track_val is not None:\n",
    "                print('Early stopping checking ...')\n",
    "                if self.track_validation(step, lv, session, saver, outp) is True:\n",
    "                    print('Early stopping: Validation loss has not improved' + \n",
    "                      ' for {} steps. Last best model has been stored'.format(self.early))\n",
    "                    return\n",
    "\n",
    "        # Save checkpoint of the model each 1000 steps (default)\n",
    "        if step % self.checkp_interval == 0:\n",
    "            store_checkpoint(session, saver, step, outp)\n",
    "        \n",
    "        \n",
    "    def print_loss(self, loss_value, step, label, duration):\n",
    "        \"\"\" Prints loss according at given step\"\"\"\n",
    "        examples_per_sec = self.opts['batch_size'] / duration\n",
    "        format_str = ('%s: step %d, %s loss = %.2f (%.1f examples/sec; %.3f '\n",
    "            'sec/batch)')\n",
    "        print (format_str % (dt.datetime.now(), step, label, loss_value,\n",
    "            examples_per_sec, float(duration)))\n",
    "\n",
    "    def display_loss(self):\n",
    "        \"\"\" Prints the training and the validation evolution \"\"\"\n",
    "        x = np.arange(0, len(self.train_loss)) * self.loss_interval\n",
    "        t = plt.plot(x, self.train_loss, label='training')\n",
    "        v = plt.plot(x, self.val_loss, label='validation')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "            \n",
    "class PairedSiamese(SiameseNetwork):\n",
    "    \n",
    "    \n",
    "    def get_label_batch(self, offset, batch_size):\n",
    "        return self.tr_labels[offset:(offset + batch_size)]\n",
    "    \n",
    "    def get_placeholder_suffixes(self):\n",
    "        return ['left', 'right']\n",
    "    \n",
    "    def get_loss(self, placeholders, labels):\n",
    "        left, right = placeholders\n",
    "        logits1 = self.model(left, 'left')\n",
    "        logits2 = self.model(right, 'right')\n",
    "        return self._paired_loss(logits1, logits2, labels)\n",
    "    \n",
    "    def _paired_loss(self, f1, f2, labels, thao = 1):\n",
    "        \"\"\" Loss for a evaluation of a pair of (non)-corresponding slices\n",
    "            Paired loss L:\n",
    "                L = 0.5 * d(f1, f2) if positive example\n",
    "                L = 0.5 * max(0 , thao - d(f1, f2)) if negative example\n",
    "            Where thao is usually 1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Must be float, convert to be sure\n",
    "        dists = euclidean_dist(f1, f2)\n",
    "        \n",
    "        # Masks of classes\n",
    "        int_labels = tf.cast(labels, tf.int32)\n",
    "        pos_ind = tf.cast(labels, tf.float32)\n",
    "        neg_ind = tf.cast(tf.sub(tf.ones(labels.get_shape(), tf.int32), int_labels), tf.float32)\n",
    "        \n",
    "        # Apply positive and negative to both classes\n",
    "        all_pos = dists\n",
    "        all_neg = tf.sub(tf.constant([thao], shape=[dists.get_shape()[0]], dtype=dists.dtype), dists)\n",
    "        all_neg = tf.maximum(tf.zeros(all_neg.get_shape(), tf.float32), all_neg)\n",
    "\n",
    "        # Apply binary mask to both so set 0's where not correct\n",
    "        binary_pos = tf.mul(pos_ind, all_pos)\n",
    "        binary_neg = tf.mul(neg_ind, all_neg)\n",
    "        \n",
    "        # Sum both and multiply by 1/2\n",
    "        summed = tf.add(binary_pos, binary_neg)\n",
    "        summed = tf.mul(summed, tf.constant([0.5], shape=[summed.get_shape()[0]]))\n",
    "        return tf.reduce_mean(summed)\n",
    "\n",
    "\n",
    "class TripletSiamese(SiameseNetwork):\n",
    "    \n",
    "    def get_label_batch(self, offset, batch_size):\n",
    "        # Return whatever since it is not used\n",
    "        return np.empty((batch_size))\n",
    "    \n",
    "    def get_placeholder_suffixes(self):\n",
    "        return ['left', 'center' 'right']\n",
    "    \n",
    "    def get_loss(self, placeholders, labels):\n",
    "        left, center, right = placeholders\n",
    "        logits1 = self.model(left, 'left')\n",
    "        logits2 = self.model(center, 'center')\n",
    "        logits3 = self.model(right, 'right')\n",
    "        return self._triplet_loss(logits1, logits2, logits3)\n",
    "    \n",
    "    def _triplet_loss(self, f1, f2, f3, alpha = 1):\n",
    "        \"\"\" Loss for a evaluation of a pair of (non)-corresponding slices\n",
    "            Triplet loss L:\n",
    "                L= 0.5 * max(0, d(f1, f2) - d(f1, f3) + alpha)\n",
    "            Where alpha is usually 1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Distances negative and positive\n",
    "        dist_pos = euclidean_dist(f1, f2)\n",
    "        dist_neg = euclidean_dist(f1, f3)\n",
    "\n",
    "        # Get maximum \n",
    "        subs = tf.sub(dist_pos, dist_neg)\n",
    "        added = tf.add(subs, tf.constant([alpha], shape=[subs.get_shape()[0]], dtype=subs.dtype))\n",
    "        loss = tf.maximum(tf.zeros(added.get_shape(), added.dtype), added)\n",
    "        \n",
    "        # Multiply by 1/2\n",
    "        const = tf.constant([0.5], shape=[loss.get_shape()[0]])\n",
    "        weighted = tf.mul(loss, const)\n",
    "        return tf.reduce_mean(weighted)\n",
    "\n",
    "\n",
    "def reshape_data(x):\n",
    "    \"\"\" Converts the data into the Tensorflow format with float32 type\"\"\"\n",
    "    return np.moveaxis(x, 2, 4).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "no\n",
      "yes\n",
      "no\n",
      "yes\n",
      "no\n",
      "yes\n",
      "no\n",
      "yes\n",
      "no\n",
      "2016-07-08 13:11:10.323486: step 10, training loss = 3.06 (53.9 examples/sec; 2.375 sec/batch)\n",
      "2016-07-08 13:11:10.323630: step 10, validation loss = 3.14 (40.6 examples/sec; 3.150 sec/batch)\n",
      "Early stopping checking ...\n"
     ]
    }
   ],
   "source": [
    "opts = {'batch_size': 128, 'ksize': 7, 'maps':25, 'maxp': True, 'lrn': False, 'lr': 1e-4,\n",
    "        'steps': 10, 'logs': \"/home/morad/tf/logs\", 'dropout': 1, 'hiddens1': 100, 'hiddens2': 100,\n",
    "       'early': 5, 'gpu': False}\n",
    "siamese = PairedSiamese(opts)\n",
    "#siamese.read_data('/DataDisk/morad/cremi/datasets/dataset.h5')\n",
    "siamese.read_data('/home/morad/projects/neural/neuralimg/tests/out.h5')\n",
    "siamese.train('/home/morad/tf/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
